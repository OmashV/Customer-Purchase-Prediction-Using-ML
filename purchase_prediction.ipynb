{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gsPN3M0ZGuI5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKiz_HrGIsu3",
        "outputId": "bbe3e402-dd58-4d7b-a688-b83a24091cb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CUSTOMER PURCHASE PREDICTION ANALYSIS\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Customer Purchase Prediction - AI/ML Engineer Assignment\n",
        "# Predicting whether a customer will make a purchase in the next 30 days\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV,StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
        "import shap\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CUSTOMER PURCHASE PREDICTION ANALYSIS\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Loading and Exploration\n",
        "\n",
        "print(\"\\n1. LOADING AND EXPLORING DATA\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('customer_data.csv')\n",
        "print(f\"Dataset shape: {df.shape}\\n\")\n",
        "\n",
        "# Preview\n",
        "print(\"First 5 rows:\")\n",
        "print(df.head(), \"\\n\")\n",
        "\n",
        "# Info\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info(), \"\\n\")\n",
        "\n",
        "# Missing values\n",
        "missing = df.isnull().sum()\n",
        "missing_percent = (missing / len(df)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Missing Percentage': missing_percent\n",
        "}).sort_values('Missing Count', ascending=False)\n",
        "print(\"Missing Values:\")\n",
        "print(missing_df[missing_df['Missing Count'] > 0], \"\\n\")\n",
        "\n",
        "# Target distribution\n",
        "target_dist = df['is_purchased_next_30'].value_counts()\n",
        "target_percent = df['is_purchased_next_30'].value_counts(normalize=True) * 100\n",
        "print(\"Target Variable Distribution:\")\n",
        "print(f\"Class 0 (No Purchase): {target_dist[0]} ({target_percent[0]:.1f}%)\")\n",
        "print(f\"Class 1 (Purchase): {target_dist[1]} ({target_percent[1]:.1f}%)\\n\")\n",
        "\n",
        "# Numerical summary\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "print(\"Numerical Features Summary:\")\n",
        "print(df[num_cols].describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8UnrV-FIzCg",
        "outputId": "5a058cda-bd86-4f3a-8da4-df40bf10ffed"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. LOADING AND EXPLORING DATA\n",
            "----------------------------------------\n",
            "Dataset shape: (5000, 13)\n",
            "\n",
            "First 5 rows:\n",
            "  customer_id signup_date last_activity_date last_purchase_date  total_spent  \\\n",
            "0     C100000  2023-08-02         2024-10-12         2023-10-27       397.81   \n",
            "1     C100001  2020-08-16         2020-11-13                NaN         0.00   \n",
            "2     C100002  2020-02-21         2020-12-18         2020-08-28        33.25   \n",
            "3     C100003  2024-02-27         2026-06-19         2025-09-15        77.56   \n",
            "4     C100004  2021-07-17         2023-05-25                NaN         0.00   \n",
            "\n",
            "   avg_cart_value  num_visits_30d  num_purchases_90d  \\\n",
            "0           22.76               0                  0   \n",
            "1            0.00               0                  0   \n",
            "2           51.38               1                  1   \n",
            "3           32.41               1                  2   \n",
            "4            0.00               2                  0   \n",
            "\n",
            "   days_since_last_purchase preferred_device     city    country  \\\n",
            "0                       643           mobile    Kandy  Sri Lanka   \n",
            "1                      1810          desktop   Matara  Sri Lanka   \n",
            "2                      1798           mobile  Colombo  Sri Lanka   \n",
            "3                       -46          desktop   Jaffna  Sri Lanka   \n",
            "4                      1475           mobile    Galle  Sri Lanka   \n",
            "\n",
            "   is_purchased_next_30  \n",
            "0                     0  \n",
            "1                     0  \n",
            "2                     0  \n",
            "3                     1  \n",
            "4                     0   \n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 13 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   customer_id               5000 non-null   object \n",
            " 1   signup_date               5000 non-null   object \n",
            " 2   last_activity_date        5000 non-null   object \n",
            " 3   last_purchase_date        3004 non-null   object \n",
            " 4   total_spent               5000 non-null   float64\n",
            " 5   avg_cart_value            5000 non-null   float64\n",
            " 6   num_visits_30d            5000 non-null   int64  \n",
            " 7   num_purchases_90d         5000 non-null   int64  \n",
            " 8   days_since_last_purchase  5000 non-null   int64  \n",
            " 9   preferred_device          5000 non-null   object \n",
            " 10  city                      5000 non-null   object \n",
            " 11  country                   5000 non-null   object \n",
            " 12  is_purchased_next_30      5000 non-null   int64  \n",
            "dtypes: float64(2), int64(4), object(7)\n",
            "memory usage: 507.9+ KB\n",
            "None \n",
            "\n",
            "Missing Values:\n",
            "                    Missing Count  Missing Percentage\n",
            "last_purchase_date           1996               39.92 \n",
            "\n",
            "Target Variable Distribution:\n",
            "Class 0 (No Purchase): 4060 (81.2%)\n",
            "Class 1 (Purchase): 940 (18.8%)\n",
            "\n",
            "Numerical Features Summary:\n",
            "       total_spent  avg_cart_value  num_visits_30d  num_purchases_90d  \\\n",
            "count  5000.000000     5000.000000     5000.000000        5000.000000   \n",
            "mean     86.162520       26.727420        1.988400           0.722800   \n",
            "std     116.037624       26.619687        1.420799           1.044691   \n",
            "min       0.000000        0.000000        0.000000           0.000000   \n",
            "25%       0.000000        0.000000        1.000000           0.000000   \n",
            "50%      48.765000       24.960000        2.000000           0.000000   \n",
            "75%     127.827500       48.570000        3.000000           1.000000   \n",
            "max    1024.070000      130.980000        8.000000           6.000000   \n",
            "\n",
            "       days_since_last_purchase  is_purchased_next_30  \n",
            "count               5000.000000           5000.000000  \n",
            "mean                1040.050400              0.188000  \n",
            "std                  527.131029              0.390751  \n",
            "min                 -440.000000              0.000000  \n",
            "25%                  620.000000              0.000000  \n",
            "50%                 1043.500000              0.000000  \n",
            "75%                 1470.250000              0.000000  \n",
            "max                 2036.000000              1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Data Preprocessing and Feature Engineering\n",
        "\n",
        "\n",
        "def preprocess_data(df):\n",
        "    print(\"\\n2. DATA PREPROCESSING AND FEATURE ENGINEERING\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    df_processed = df.copy()\n",
        "\n",
        "    # Parse date columns\n",
        "    for col in ['signup_date', 'last_activity_date', 'last_purchase_date']:\n",
        "        df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce')\n",
        "\n",
        "    # Customers who purchased before\n",
        "    df_processed['has_purchased_before'] = df_processed['last_purchase_date'].notna().astype(int)\n",
        "\n",
        "    # Reference date for time-based features\n",
        "    reference_date = pd.concat([df_processed[c] for c in ['signup_date', 'last_activity_date', 'last_purchase_date']]).max() + pd.Timedelta(days=1)\n",
        "    df_processed['days_since_signup'] = (reference_date - df_processed['signup_date']).dt.days\n",
        "    df_processed['days_since_last_activity'] = (reference_date - df_processed['last_activity_date']).dt.days\n",
        "\n",
        "    # Handle last purchase for customers who never purchased\n",
        "    df_processed['days_since_last_purchase_filled'] = df_processed['days_since_last_purchase'].fillna(df_processed['days_since_signup'])\n",
        "\n",
        "    # RFM features\n",
        "    df_processed['recency'] = df_processed['days_since_last_purchase_filled']\n",
        "    df_processed['frequency'] = df_processed['num_purchases_90d']\n",
        "    df_processed['monetary'] = df_processed['total_spent']\n",
        "\n",
        "    df_processed['recency_score'] = pd.qcut(df_processed['recency'].rank(method='first'), 5, labels=[5,4,3,2,1]).astype(int)\n",
        "    df_processed['frequency_score'] = pd.qcut(df_processed['frequency'].rank(method='first'), 5, labels=[1,2,3,4,5]).astype(int)\n",
        "    df_processed['monetary_score'] = pd.qcut(df_processed['monetary'].rank(method='first'), 5, labels=[1,2,3,4,5]).astype(int)\n",
        "    df_processed['rfm_score'] = (df_processed['recency_score'].astype(str) +\n",
        "                                 df_processed['frequency_score'].astype(str) +\n",
        "                                 df_processed['monetary_score'].astype(str)).astype(int)\n",
        "\n",
        "    # Customer engagement features\n",
        "    df_processed['customer_age_days'] = df_processed['days_since_signup']\n",
        "    df_processed['activity_recency'] = df_processed['days_since_last_activity']\n",
        "    df_processed['visits_per_day'] = df_processed['num_visits_30d'] / 30\n",
        "    df_processed['purchase_frequency'] = df_processed['num_purchases_90d'] / 90\n",
        "    df_processed['avg_days_between_purchases'] = np.where(\n",
        "        df_processed['num_purchases_90d'] > 0,\n",
        "        90 / df_processed['num_purchases_90d'],\n",
        "        df_processed['days_since_signup']\n",
        "    )\n",
        "\n",
        "    # Customer segments\n",
        "    df_processed['is_high_value'] = (df_processed['total_spent'] > df_processed['total_spent'].quantile(0.8)).astype(int)\n",
        "    df_processed['is_frequent_visitor'] = (df_processed['num_visits_30d'] > df_processed['num_visits_30d'].quantile(0.7)).astype(int)\n",
        "    df_processed['is_recent_purchaser'] = (df_processed['days_since_last_purchase_filled'] < 30).astype(int)\n",
        "\n",
        "    # Encode categorical variables\n",
        "    for col in ['preferred_device', 'city', 'country']:\n",
        "        df_processed[f'{col}_encoded'] = LabelEncoder().fit_transform(df_processed[col])\n",
        "\n",
        "    # Device dummies\n",
        "    df_processed = pd.concat([df_processed, pd.get_dummies(df_processed['preferred_device'], prefix='device')], axis=1)\n",
        "\n",
        "    print(f\"Processed dataset shape: {df_processed.shape}\")\n",
        "    print(\"New features created successfully!\")\n",
        "\n",
        "    return df_processed\n",
        "\n"
      ],
      "metadata": {
        "id": "tfSUGNRoJIgK"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute Preprocessing\n",
        "df_processed = preprocess_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQTeCpacJMVE",
        "outputId": "fb76eff9-2cb0-453d-bc0a-8ee6b133bf6d"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2. DATA PREPROCESSING AND FEATURE ENGINEERING\n",
            "--------------------------------------------------\n",
            "Processed dataset shape: (5000, 38)\n",
            "New features created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Feature Selection and Preparation\n",
        "\n",
        "def prepare_features(df_processed):\n",
        "    \"\"\"Select and prepare features for modeling\"\"\"\n",
        "    print(\"\\n3. FEATURE SELECTION AND PREPARATION\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # feature columns\n",
        "    feature_columns = [\n",
        "\n",
        "        'total_spent', 'avg_cart_value', 'num_visits_30d', 'num_purchases_90d',\n",
        "        'days_since_last_purchase_filled', 'days_since_signup', 'days_since_last_activity',\n",
        "        'recency_score', 'frequency_score', 'monetary_score',\n",
        "        'customer_age_days', 'activity_recency', 'visits_per_day', 'purchase_frequency', 'avg_days_between_purchases',\n",
        "        'has_purchased_before', 'is_high_value', 'is_frequent_visitor', 'is_recent_purchaser',\n",
        "        'preferred_device_encoded', 'city_encoded',\n",
        "        'device_desktop', 'device_mobile', 'device_tablet'\n",
        "    ]\n",
        "\n",
        "\n",
        "    X = df_processed[feature_columns].copy()\n",
        "    y = df_processed['is_purchased_next_30'].copy()\n",
        "\n",
        "    # Handle infinite values\n",
        "    X = X.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Fill remaining missing values\n",
        "    X = X.fillna(X.median())\n",
        "\n",
        "    print(f\"Feature matrix shape: {X.shape}\")\n",
        "    print(f\"Target vector shape: {y.shape}\")\n",
        "    print(f\"Features selected: {len(feature_columns)}\")\n",
        "\n",
        "    return X, y, feature_columns\n",
        "\n",
        "# Prepare features\n",
        "X, y, feature_names = prepare_features(df_processed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMMlcHtPJN6L",
        "outputId": "51077778-aa5d-40c5-d6af-7756db562b7d"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. FEATURE SELECTION AND PREPARATION\n",
            "----------------------------------------\n",
            "Feature matrix shape: (5000, 24)\n",
            "Target vector shape: (5000,)\n",
            "Features selected: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Model Training and Evaluation\n",
        "\n",
        "def train_models_cv(X, y, feature_names):\n",
        "\n",
        "    print(\"\\nMODEL TRAINING AND EVALUATION (WITH CV & HYPERPARAMETER TUNING)\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # Split data (for final test evaluation)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "    # Scale features for Logistic Regression\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Define models and hyperparameter grids\n",
        "    models = {\n",
        "        'Logistic Regression': {\n",
        "            'model': LogisticRegression(random_state=42, max_iter=1000),\n",
        "            'params': {'C': [0.01, 0.1, 1, 10]}\n",
        "        },\n",
        "        'Random Forest': {\n",
        "            'model': RandomForestClassifier(random_state=42),\n",
        "            'params': {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10]}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    trained_models = {}\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    for name, m in models.items():\n",
        "        print(f\"\\nTuning and training {name}...\")\n",
        "        if name == 'Logistic Regression':\n",
        "            grid = GridSearchCV(m['model'], m['params'], cv=cv, scoring='roc_auc')\n",
        "            grid.fit(X_train_scaled, y_train)\n",
        "            best_model = grid.best_estimator_\n",
        "            y_pred = best_model.predict(X_test_scaled)\n",
        "            y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
        "            trained_models[name] = {'model': best_model, 'scaler': scaler}\n",
        "        else:\n",
        "            grid = GridSearchCV(m['model'], m['params'], cv=cv, scoring='roc_auc')\n",
        "            grid.fit(X_train, y_train)\n",
        "            best_model = grid.best_estimator_\n",
        "            y_pred = best_model.predict(X_test)\n",
        "            y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "            trained_models[name] = {'model': best_model, 'scaler': None}\n",
        "\n",
        "        # Calculate metrics\n",
        "        results[name] = {\n",
        "            'accuracy': accuracy_score(y_test, y_pred),\n",
        "            'precision': precision_score(y_test, y_pred),\n",
        "            'recall': recall_score(y_test, y_pred),\n",
        "            'f1': f1_score(y_test, y_pred),\n",
        "            'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
        "        }\n",
        "\n",
        "        print(f\"Best hyperparameters: {grid.best_params_}\")\n",
        "\n",
        "    # Comparison\n",
        "    results_df = pd.DataFrame(results).T\n",
        "    print(\"\\nMODEL COMPARISON:\")\n",
        "    print(results_df.round(4))\n",
        "\n",
        "    # --- Save model comparison results ---\n",
        "    results_df.to_csv(\"model_comparison.csv\")\n",
        "    print(\"Model comparison saved to 'model_comparison.csv'\")\n",
        "\n",
        "    best_model_name = results_df['roc_auc'].idxmax()\n",
        "    print(f\"\\nBest model based on ROC-AUC: {best_model_name}\")\n",
        "\n",
        "    return trained_models, results_df, best_model_name, X_test, y_test, scaler\n"
      ],
      "metadata": {
        "id": "Ll6dKEWhJSPw"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate models\n",
        "trained_models, results_df, best_model_name, X_test, y_test, scaler = train_models_cv(X, y, feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLzuAmiMJXwg",
        "outputId": "53fa2320-1017-4e57-919d-641eb3762c64"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MODEL TRAINING AND EVALUATION (WITH CV & HYPERPARAMETER TUNING)\n",
            "------------------------------------------------------------\n",
            "Training set size: 4000, Test set size: 1000\n",
            "\n",
            "Tuning and training Logistic Regression...\n",
            "Best hyperparameters: {'C': 0.01}\n",
            "\n",
            "Tuning and training Random Forest...\n",
            "Best hyperparameters: {'max_depth': 5, 'n_estimators': 200}\n",
            "\n",
            "MODEL COMPARISON:\n",
            "                     accuracy  precision  recall      f1  roc_auc\n",
            "Logistic Regression     0.817     0.6667  0.0532  0.0985   0.7021\n",
            "Random Forest           0.812     0.5000  0.0053  0.0105   0.6921\n",
            "Model comparison saved to 'model_comparison.csv'\n",
            "\n",
            "Best model based on ROC-AUC: Logistic Regression\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Model Interpretability\n",
        "def analyze_feature_importance(trained_models, best_model_name, X_test, feature_names):\n",
        "\n",
        "    print(\"\\n5. MODEL INTERPRETABILITY\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    best_model = trained_models[best_model_name]['model']\n",
        "\n",
        "    # Get feature importance\n",
        "    if best_model_name == 'Logistic Regression':\n",
        "        importance = np.abs(best_model.coef_[0])\n",
        "    else:\n",
        "        importance = best_model.feature_importances_\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"\\nTop 10 Most Important Features ({best_model_name}):\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    # SHAP analysis (sampled)\n",
        "    shap_feature_importance = None\n",
        "    try:\n",
        "        X_sample = X_test.sample(min(100, len(X_test)), random_state=42)\n",
        "        explainer = shap.LinearExplainer(best_model, X_sample) if best_model_name == 'Logistic Regression' else shap.TreeExplainer(best_model)\n",
        "        shap_values = explainer.shap_values(X_sample)\n",
        "        shap_importance = np.abs(shap_values).mean(0)\n",
        "        shap_feature_importance = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'shap_importance': shap_importance\n",
        "        }).sort_values('shap_importance', ascending=False)\n",
        "\n",
        "        print(\"\\nTop 10 Features by SHAP Importance:\")\n",
        "        print(shap_feature_importance.head(10))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"SHAP analysis failed: {e}\")\n",
        "\n",
        "    # Key insights with short explanations for top 3 features\n",
        "    top_features = feature_importance.head(3)['feature'].tolist()\n",
        "    print(\"\\nKEY INSIGHTS:\")\n",
        "    for i, feature in enumerate(top_features, 1):\n",
        "        fname = feature.lower()\n",
        "        if 'recency' in fname or 'days_since' in fname:\n",
        "            explanation = \"Recent activity or purchase is predictive of buying again.\"\n",
        "        elif 'frequency' in fname or 'purchases' in fname:\n",
        "            explanation = \"Frequent purchasers are more likely to buy again.\"\n",
        "        elif 'monetary' in fname or 'spent' in fname or 'total' in fname:\n",
        "            explanation = \"Higher spending indicates a valuable customer.\"\n",
        "        elif 'visit' in fname:\n",
        "            explanation = \"More visits correlate with higher purchase probability.\"\n",
        "        else:\n",
        "            explanation = \"This feature influences purchase behavior.\"\n",
        "        print(f\"{i}. Feature '{feature}': {explanation}\")\n",
        "\n",
        "    return feature_importance, shap_feature_importance\n",
        "\n",
        "# Call the function\n",
        "feature_importance, shap_importance = analyze_feature_importance(\n",
        "    trained_models, best_model_name, X_test, feature_names\n",
        ")\n",
        "\n",
        "# Save feature importance to CSV\n",
        "feature_importance.to_csv(\"feature_importance.csv\", index=False)\n",
        "print(\"Feature importance saved to 'feature_importance.csv'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsHMx6xFJZZ6",
        "outputId": "c7e1b298-760f-46af-a652-ca8c96782053"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5. MODEL INTERPRETABILITY\n",
            "------------------------------\n",
            "\n",
            "Top 10 Most Important Features (Logistic Regression):\n",
            "                            feature  importance\n",
            "14       avg_days_between_purchases    0.249272\n",
            "12                   visits_per_day    0.176494\n",
            "2                    num_visits_30d    0.176494\n",
            "8                   frequency_score    0.141496\n",
            "18              is_recent_purchaser    0.110562\n",
            "1                    avg_cart_value    0.090042\n",
            "4   days_since_last_purchase_filled    0.068729\n",
            "3                 num_purchases_90d    0.065333\n",
            "13               purchase_frequency    0.065333\n",
            "15             has_purchased_before    0.054067\n",
            "\n",
            "Top 10 Features by SHAP Importance:\n",
            "                            feature shap_importance\n",
            "14       avg_days_between_purchases      209.316401\n",
            "4   days_since_last_purchase_filled       31.141613\n",
            "6          days_since_last_activity       10.832151\n",
            "11                 activity_recency       10.832151\n",
            "5                 days_since_signup        4.872259\n",
            "10                customer_age_days        4.872259\n",
            "0                       total_spent        4.349805\n",
            "1                    avg_cart_value        2.167614\n",
            "2                    num_visits_30d        0.203604\n",
            "8                   frequency_score        0.169937\n",
            "\n",
            "KEY INSIGHTS:\n",
            "1. Feature 'avg_days_between_purchases': Frequent purchasers are more likely to buy again.\n",
            "2. Feature 'visits_per_day': More visits correlate with higher purchase probability.\n",
            "3. Feature 'num_visits_30d': More visits correlate with higher purchase probability.\n",
            "Feature importance saved to 'feature_importance.csv'\n"
          ]
        }
      ]
    }
  ]
}